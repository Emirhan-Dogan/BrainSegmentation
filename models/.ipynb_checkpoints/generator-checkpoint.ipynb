{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c589188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdaptiveInstanceNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1,affine=True):\n",
    "        super(AdaptiveInstanceNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # weight and bias are dynamically assigned\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        # just dummy buffers, not used\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.weight is not None and self.bias is not None, \"Please assign weight and bias before calling AdaIN!\"\n",
    "        b, c = x.size(0), x.size(1)\n",
    "        running_mean = self.running_mean.repeat(b)\n",
    "        running_var = self.running_var.repeat(b)\n",
    "\n",
    "        x_reshaped = x.contiguous().view(1, b * c, *x.size()[2:])\n",
    "\n",
    "        out = F.batch_norm(\n",
    "            x_reshaped, running_mean, running_var, self.weight+1, self.bias,\n",
    "            True, self.momentum, self.eps)\n",
    "        return out.view(b, c, *x.size()[2:])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + str(self.num_features) + ')'\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, affine=True):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n",
    "            self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = [-1] + [1] * (x.dim() - 1)\n",
    "        # print(x.size())\n",
    "        if x.size(0) == 1:\n",
    "            # These two lines run much faster in pytorch 0.4 than the two lines listed below.\n",
    "            mean = x.view(-1).mean().view(*shape)\n",
    "            std = x.view(-1).std().view(*shape)\n",
    "        else:\n",
    "            mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "            std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "\n",
    "        if self.affine:\n",
    "            shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
    "        return x\n",
    "\n",
    "class ResBlk(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, planes, norm_layer):\n",
    "        super(ResBlk, self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(planes,planes,(3,3),(1,1),(1,1))\n",
    "        self.norm1 = norm_layer(planes, affine=True)\n",
    "\n",
    "        self.conv2 =nn.Conv2d(planes,planes,(3,3),(1,1),(1,1))\n",
    "        self.norm2 = norm_layer(planes, affine=True)\n",
    "\n",
    "        self.act = nn.LeakyReLU(0.2,True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.norm1(x)\n",
    "        out = self.act(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.norm2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out += identity\n",
    "        return out / math.sqrt(2)\n",
    "\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self,  style_dim=8):\n",
    "        super(StyleEncoder, self).__init__()\n",
    "        nef=32\n",
    "        act = nn.LeakyReLU(0.2,True)\n",
    "        main = []\n",
    "        main +=  [nn.Conv2d(1, nef, (7, 7), (1, 1), (3, 3)), nn.InstanceNorm2d(nef, affine=True), act]\n",
    "        main += [nn.Conv2d(nef, nef * 2, (4, 4), (2, 2), (1, 1)), nn.InstanceNorm2d(nef * 2, affine=True), act]\n",
    "        main += [nn.Conv2d(nef * 2, nef * 4, (4, 4), (2, 2), (1, 1)), nn.InstanceNorm2d(nef * 4, affine=True), act]\n",
    "        main += [nn.Conv2d(nef * 4, nef * 4, (4, 4), (2, 2), (1, 1)), nn.InstanceNorm2d(nef * 4, affine=True), act]\n",
    "        main += [nn.Conv2d(nef * 4, nef * 4, (4, 4), (2, 2), (1, 1)), nn.InstanceNorm2d(nef * 4, affine=True), act]\n",
    "        main += [nn.Conv2d(nef * 4, nef * 4, (4, 4), (2, 2), (1, 1)), nn.InstanceNorm2d(nef * 4, affine=True), act]\n",
    "        main += [nn.AdaptiveAvgPool2d(1)] # global average pooling\n",
    "        main += [nn.Conv2d(nef * 4, style_dim, 1, 1, 0)]\n",
    "        self.model = nn.Sequential(*main)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "class ContentEncoder(nn.Module):\n",
    "    def __init__(self, nef=32, norm=nn.InstanceNorm2d):\n",
    "        super(ContentEncoder, self).__init__()\n",
    "        main = []\n",
    "        main += [nn.Conv2d(1, nef, (7, 7), (1, 1), (3, 3)), nn.InstanceNorm2d(nef, affine=True),nn.LeakyReLU(0.2,True)]\n",
    "        main += [nn.Conv2d(nef, nef * 2, (4, 4), (2, 2), (1, 1)), nn.InstanceNorm2d(nef * 2, affine=True), nn.LeakyReLU(0.2,True)]\n",
    "        main += [nn.Conv2d(nef * 2, nef * 4, (4, 4), (2, 2), (1, 1)), nn.InstanceNorm2d(nef * 4, affine=True), nn.LeakyReLU(0.2,True)]\n",
    "        main += [ResBlk(nef * 4, norm_layer=norm)]\n",
    "        main += [ResBlk(nef * 4, norm_layer=norm)]\n",
    "        main += [ResBlk(nef * 4, norm_layer=norm)]\n",
    "        main += [ResBlk(nef * 4, norm_layer=norm)]\n",
    "        self.main = nn.Sequential(*main)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ndf=32,style_dim=8):\n",
    "        super(Decoder, self).__init__()\n",
    "        main = []\n",
    "        main += [ResBlk(ndf * 4, norm_layer=AdaptiveInstanceNorm2d)]\n",
    "        main += [ResBlk(ndf * 4, norm_layer=AdaptiveInstanceNorm2d)]\n",
    "        main += [ResBlk(ndf * 4, norm_layer=AdaptiveInstanceNorm2d)]\n",
    "        main += [ResBlk(ndf * 4, norm_layer=AdaptiveInstanceNorm2d)]\n",
    "\n",
    "        main += [AdaptiveInstanceNorm2d(ndf * 4), nn.LeakyReLU(0.2,True)]\n",
    "        main += [nn.Upsample(scale_factor=2), nn.Conv2d(ndf * 4, ndf * 2, (5, 5), (1, 1), (2, 2)),\n",
    "                 AdaptiveInstanceNorm2d(ndf * 2), nn.LeakyReLU(0.2,True)]\n",
    "        main += [nn.Upsample(scale_factor=2), nn.Conv2d(ndf * 2, ndf * 1, (5, 5), (1, 1), (2, 2)),\n",
    "                 AdaptiveInstanceNorm2d(ndf), nn.LeakyReLU(0.2,True)]\n",
    "        main += [nn.Conv2d(ndf, 1, (7, 7), (1, 1), (3, 3)), nn.Tanh()]\n",
    "        self.main = nn.Sequential(*main)\n",
    "\n",
    "        mlp = []\n",
    "        mlp += [nn.Linear(style_dim, 128),nn.ReLU(True)]\n",
    "        mlp += [nn.Linear(128, 256), nn.ReLU(True)]\n",
    "        mlp += [nn.Linear(256, self.get_num_adain_params(self.main))]\n",
    "        self.mlp=nn.Sequential(*mlp)\n",
    "\n",
    "    def assign_adain_params(self, adain_params, model):\n",
    "        # assign the adain_params to the AdaIN layers in model\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
    "                mean = adain_params[:, :m.num_features]\n",
    "                std = adain_params[:, m.num_features:2*m.num_features]\n",
    "                m.bias = mean.contiguous().view(-1)\n",
    "                m.weight = std.contiguous().view(-1)\n",
    "                if adain_params.size(1) > 2*m.num_features:\n",
    "                    adain_params = adain_params[:, 2*m.num_features:]\n",
    "\n",
    "    def get_num_adain_params(self, model):\n",
    "        # return the number of AdaIN parameters needed by the model\n",
    "        num_adain_params = 0\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
    "                num_adain_params += 2*m.num_features\n",
    "        return num_adain_params\n",
    "\n",
    "    def forward(self,c, s):\n",
    "        adain_params=self.mlp(s)\n",
    "        self.assign_adain_params(adain_params,self.main)\n",
    "        return self.main(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
