{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09bd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from torch.utils.data import DataLoader\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from utils import SEGDataset\n",
    "from models import DARUnet\n",
    "import time\n",
    "from utils.tta import PatchInferencer\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None,weight=None, ignore_index=255, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.size_average = size_average\n",
    "        self.weight=weight\n",
    "\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor):\n",
    "        i = input\n",
    "        t = target\n",
    "\n",
    "        # Change the shape of input and target to B x N x num_voxels.\n",
    "        i = i.view(i.size(0), i.size(1), -1)\n",
    "        t = t.view(t.size(0), t.size(1), -1)\n",
    "\n",
    "        # Compute the log proba.\n",
    "        logpt = F.log_softmax(i, dim=1)\n",
    "        # Get the proba\n",
    "        pt = torch.exp(logpt)  # B,H*W or B,N,H*W\n",
    "\n",
    "        if self.weight is not None:\n",
    "            class_weight = torch.as_tensor(self.weight)\n",
    "            class_weight = class_weight.to(i)\n",
    "\n",
    "            at = class_weight[None, :, None]\n",
    "            at = at.expand((t.size(0), -1, t.size(2)))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        # Compute the loss mini-batch.\n",
    "        weight = torch.pow(-pt + 1.0, self.gamma)\n",
    "        loss = torch.mean(-weight * t * logpt, dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def _one_hot_encoder(self, input_tensor):\n",
    "        tensor_list = []\n",
    "        for i in range(self.n_classes):\n",
    "            temp_prob = input_tensor == i  # * torch.ones_like(input_tensor)\n",
    "            tensor_list.append(temp_prob.unsqueeze(1))\n",
    "        output_tensor = torch.cat(tensor_list, dim=1)\n",
    "        return output_tensor.float()\n",
    "\n",
    "    def _dice_loss(self, score, target):\n",
    "        target = target.float()\n",
    "        smooth = 1e-5\n",
    "        intersect = torch.sum(score * target)\n",
    "        y_sum = torch.sum(target * target)\n",
    "        z_sum = torch.sum(score * score)\n",
    "        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "        loss = 1 - loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, target, weight=None, softmax=False):\n",
    "        if softmax:\n",
    "            inputs = torch.softmax(inputs, dim=1)\n",
    "        if weight is None:\n",
    "            weight = [1] * self.n_classes\n",
    "        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n",
    "        class_wise_dice = []\n",
    "        loss = 0.0\n",
    "        for i in range(0, self.n_classes):\n",
    "            dice = self._dice_loss(inputs[:, i], target[:, i])\n",
    "            class_wise_dice.append(1.0 - dice.item())\n",
    "            loss += dice * weight[i]\n",
    "        return loss / self.n_classes\n",
    "\n",
    "def dice_loss_chill(output, gt):\n",
    "    num = (output*gt).sum(dim=[2, 3, 4])\n",
    "    denom = output.sum(dim=[2, 3, 4]) + gt.sum(dim=[2, 3, 4]) + 0.001\n",
    "    return num, denom\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--name', type=str, default='experiment')\n",
    "parser.add_argument('--train_dataroot', type=str, default='')\n",
    "parser.add_argument('--val_dataroot', type=str, default='')\n",
    "parser.add_argument('--num_classes', type=int, default=3)\n",
    "parser.add_argument('--epoch_max', type=int, default=100)\n",
    "\n",
    "\n",
    "\n",
    "opts = parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    epoch_max=opts.epoch_max\n",
    "\n",
    "    train_A_loader = DataLoader(dataset=SEGDataset(opts.train_dataroot,opts.num_classes), batch_size=2, shuffle=True, drop_last=True, num_workers=4, pin_memory=True)\n",
    "    val_A_loader = DataLoader(dataset=SEGDataset(opts.val_dataroot,opts.num_classes), batch_size=1, shuffle=True, drop_last=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    netS=DARUnet()\n",
    "    netS.cuda()\n",
    "\n",
    "    ipp = PatchInferencer(n_class=opts.num_classes,TTA=False)\n",
    "\n",
    "    optS=AdaBelief(netS.parameters(), lr=5e-4, weight_decay=1e-4, eps=1e-16, betas=(0.9, 0.999), weight_decouple=True, rectify=True, print_change_log=False)\n",
    "\n",
    "    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optS, epoch_max, eta_min=5e-7, last_epoch=-1)\n",
    "\n",
    "    DC=DiceLoss(opts.num_classes)\n",
    "    FC=FocalLoss()\n",
    "\n",
    "    iter_num=len(train_A_loader)\n",
    "    st=time.time()\n",
    "\n",
    "    print(f'start at {time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(st))}')\n",
    "    iteration=0\n",
    "    best_dice=0\n",
    "    for epoch in range(epoch_max):\n",
    "        netS.train()\n",
    "        et=time.time()\n",
    "        for iteration, train_A_data in enumerate(train_A_loader):\n",
    "            A_img=train_A_data['img'].cuda()\n",
    "            A_label=train_A_data['label'].cuda()\n",
    "\n",
    "            optS.zero_grad()\n",
    "            A_preds = netS(A_img)\n",
    "            ce_loss=0\n",
    "            dc_loss=0\n",
    "            for pred in A_preds:\n",
    "                ce_loss+=FC(pred, A_label)*10\n",
    "                dc_loss+=DC(pred, A_label, softmax=True)\n",
    "            loss_sup=ce_loss+dc_loss\n",
    "            loss_sup.backward()\n",
    "            optS.step()\n",
    "\n",
    "            sys.stdout.write('\\r[{}-{}/{}] Loss DC {:.5f} CE {:.5f} G {:.5f} D {:.5f}'.format(epoch + 1, iteration + 1, iter_num, dc_loss.item(), ce_loss.item(),  0,0))\n",
    "        scheduler.step()\n",
    "        if (epoch+1)%10==0:\n",
    "            torch.save({'seg': netS.state_dict()}, f'{epoch+1}.pt')\n",
    "        if epoch>-1:\n",
    "            netS.eval()\n",
    "            dices=[]\n",
    "            for val_data in val_A_loader:\n",
    "                for k in val_data.keys():\n",
    "                    val_data[k] = val_data[k].cuda().detach()\n",
    "                with torch.no_grad():\n",
    "                    output = ipp(netS, val_data['A_img'])\n",
    "                    pred=F.one_hot(torch.argmax(output,1), 3).permute(0,4,1,2,3)\n",
    "                    gt=val_data['A_label']\n",
    "                    num, denom = dice_loss_chill(pred,gt)\n",
    "                    d = (2 * num / denom)[:, 1:].mean().cpu().numpy()\n",
    "                    dices.append(d)\n",
    "            torch.cuda.empty_cache()\n",
    "            dices=np.mean(dices)\n",
    "            if best_dice<dices:\n",
    "                best_dice=dices\n",
    "                torch.save({'seg': netS.state_dict()}, 'best.pt')\n",
    "            time.sleep(5)\n",
    "            print(f'\\n val dice{dices} best dice {best_dice} epoch time {time.strftime(\"%M:%S\", time.localtime(time.time()-et))}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
