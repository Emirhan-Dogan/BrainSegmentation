{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30259754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ContentEncoder,Decoder,MsImageDis,content_Dis,StyleEncoder\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "class i2iSolver(nn.Module):\n",
    "    def __init__(self, opts, TTUR=True):\n",
    "        super().__init__()\n",
    "        self.opts=opts\n",
    "\n",
    "        self.enc_c=ContentEncoder()\n",
    "        self.enc_s_a=StyleEncoder(style_dim=8)\n",
    "        self.enc_s_b=StyleEncoder(style_dim=8)\n",
    "        self.dec=Decoder(style_dim=8)\n",
    "\n",
    "        self.enc_c_ema=copy.deepcopy(self.enc_c)\n",
    "        self.enc_s_a_ema=copy.deepcopy(self.enc_s_a)\n",
    "        self.enc_s_b_ema=copy.deepcopy(self.enc_s_b)\n",
    "        self.dec_ema=copy.deepcopy(self.dec)\n",
    "\n",
    "        self.dis_a=MsImageDis()\n",
    "        self.dis_b=MsImageDis()\n",
    "        self.dis_c=content_Dis()\n",
    "\n",
    "        self.gen_opt = AdaBelief(itertools.chain(self.enc_c.parameters(), self.enc_s_a.parameters(),self.enc_s_b.parameters(),self.dec.parameters()), lr=1e-4, weight_decay=0,eps=1e-16, betas=(0.5, 0.9), weight_decouple=True, rectify=True, print_change_log=False)\n",
    "        self.dis_opt = AdaBelief(itertools.chain(self.dis_a.parameters(), self.dis_b.parameters(), self.dis_c.parameters()), lr=2e-4, weight_decay=0, eps=1e-16, betas=(0.5, 0.9), weight_decouple=True, rectify=True, print_change_log=False)\n",
    "\n",
    "        self.recon_criterion=nn.L1Loss()\n",
    "\n",
    "        self.lambda_rec=10\n",
    "        self.lambda_cyc=10\n",
    "        self.lambda_fm=1\n",
    "\n",
    "        for m in self.modules():\n",
    "            if type(m) in {nn.Conv2d, nn.Linear}:\n",
    "                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def inference(self,x,r):\n",
    "        with torch.no_grad():\n",
    "            self.x = self.dec(self.enc_c(x), r)\n",
    "        return self.x\n",
    "\n",
    "    def gan_forward(self,x_a,x_b):\n",
    "        self.x_a = x_a\n",
    "        self.x_b = x_b\n",
    "\n",
    "        self.s_a = self.enc_s_a(self.x_a)\n",
    "        self.s_b = self.enc_s_b(self.x_b)\n",
    "\n",
    "        self.c_a = self.enc_c(self.x_a)\n",
    "        self.c_b = self.enc_c(self.x_b)\n",
    "\n",
    "        self.x_a_recon = self.dec(self.c_a, self.s_a)\n",
    "        self.x_b_recon = self.dec(self.c_b, self.s_b)\n",
    "\n",
    "        self.x_ab = self.dec(self.c_a, self.s_b)\n",
    "        self.x_ba = self.dec(self.c_b, self.s_a)\n",
    "\n",
    "        self.c_a_recon = self.enc_c(self.x_ab)\n",
    "        self.c_b_recon = self.enc_c(self.x_ba)\n",
    "\n",
    "        self.s_b_recon = self.enc_s_b(self.x_ab)\n",
    "        self.s_a_recon = self.enc_s_a(self.x_ba)\n",
    "\n",
    "        self.x_aba = self.dec(self.c_a_recon, self.s_a)\n",
    "        self.x_bab = self.dec(self.c_b_recon, self.s_b)\n",
    "\n",
    "    def gen_update(self):\n",
    "        self.gen_opt.zero_grad()\n",
    "\n",
    "        self.loss_g_rec = (self.recon_criterion(self.x_a, self.x_a_recon) + self.recon_criterion(self.x_b, self.x_b_recon)) * self.lambda_rec\n",
    "        self.loss_g_cyc = (self.recon_criterion(self.x_a, self.x_aba) + self.recon_criterion(self.x_b, self.x_bab)) * self.lambda_cyc\n",
    "        self.loss_g_fm = (self.recon_criterion(self.c_a, self.c_a_recon) + self.recon_criterion(self.c_b, self.c_b_recon)) *self.lambda_fm\n",
    "        self.loss_g_rec_s = (self.recon_criterion(self.s_a, self.s_a_recon) + self.recon_criterion(self.s_b, self.s_b_recon)) * 1\n",
    "        self.loss_g_adv = self.dis_a.calc_gen_loss(self.x_ba,self.x_a) + self.dis_b.calc_gen_loss(self.x_ab,self.x_b)\n",
    "        self.loss_c_adv=self.dis_c.calc_gen_loss(self.c_b,self.c_a)\n",
    "\n",
    "        self.loss_g = self.loss_g_cyc + self.loss_g_fm + self.loss_g_adv + self.loss_g_rec + self.loss_c_adv+ self.loss_g_rec_s\n",
    "        self.loss_g.backward()\n",
    "        self.gen_opt.step()\n",
    "\n",
    "        self.moving_average(self.enc_c, self.enc_c_ema, beta=0.999)\n",
    "        self.moving_average(self.enc_s_a, self.enc_s_a_ema, beta=0.999)\n",
    "        self.moving_average(self.enc_s_b, self.enc_s_b_ema, beta=0.999)\n",
    "        self.moving_average(self.dec, self.dec_ema, beta=0.999)\n",
    "\n",
    "    def dis_update(self):\n",
    "        self.dis_opt.zero_grad()\n",
    "        self.loss_dis_a = self.dis_a.calc_dis_loss(self.x_ba.detach(), self.x_a)\n",
    "        self.loss_dis_b = self.dis_b.calc_dis_loss(self.x_ab.detach(), self.x_b)\n",
    "        self.loss_dis_c= self.dis_c.calc_dis_loss(self.c_b.detach(),self.c_a.detach())\n",
    "        self.loss_dis_total = self.loss_dis_a + self.loss_dis_b+self.loss_dis_c\n",
    "        self.loss_dis_total.backward()\n",
    "        self.dis_opt.step()\n",
    "\n",
    "    def verbose(self):\n",
    "        text=''\n",
    "        for s in self.__dict__.keys():\n",
    "            if 'loss_' in s:\n",
    "                text+='{} {:.3f}  '.format(s.replace('loss_',''),getattr(self,s).item())\n",
    "        return text\n",
    "\n",
    "    def gan_visual(self,epoch):\n",
    "        collections=[]\n",
    "        for im in [self.x_a, self.x_a_recon, self.x_ab, self.x_aba, self.x_b,self.x_b_recon,self.x_ba, self.x_bab]:\n",
    "            tim= np.clip(((im[0,0].detach().cpu().numpy())+1)*127.5,0,255).astype(np.uint8)\n",
    "            collections.append(tim)\n",
    "        for i in range(2):\n",
    "            for j in range(4):\n",
    "                plt.subplot(2,4,i*4+j+1)\n",
    "                plt.imshow(collections[i*4+j],cmap='gray')\n",
    "                plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        e='%03d'%epoch\n",
    "        plt.savefig(f'{self.opts.name}/i2i_train_visual/{e}_{time.time()}.png',dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    def set_requires_grad(self, nets, requires_grad=False):\n",
    "        if not isinstance(nets, list):\n",
    "            nets = [nets]\n",
    "        for net in nets:\n",
    "            if net is not None:\n",
    "                for param in net.parameters():\n",
    "                    param.requires_grad = requires_grad\n",
    "\n",
    "    def moving_average(self, model, model_test, beta=0.999):\n",
    "        for param, param_test in zip(model.parameters(), model_test.parameters()):\n",
    "            param_test.data = torch.lerp(param.data, param_test.data, beta)\n",
    "\n",
    "    def save(self,  epoch):\n",
    "        model_name = os.path.join(self.opts.name,'i2i_checkpoints', 'enc_%04d.pt' % (epoch + 1))\n",
    "        torch.save({'enc_c': self.enc_c_ema.state_dict(), 'dec': self.dec_ema.state_dict(),\n",
    "                    'enc_s_a': self.enc_s_a_ema.state_dict(),'enc_s_b': self.enc_s_b_ema.state_dict(),\n",
    "                    'dis_a': self.dis_a.state_dict(), 'dis_b': self.dis_b.state_dict()}, model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#'enc_ema': self.enc_ema.state_dict(), 'dec_ema': self.dec_ema.state_dict(),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
