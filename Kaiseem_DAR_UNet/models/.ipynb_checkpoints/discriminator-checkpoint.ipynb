{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class feat_Dis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(feat_Dis, self).__init__()\n",
    "        model = []\n",
    "        model += [nn.Conv2d(256, 128, 3, 1, 1), nn.InstanceNorm2d(128, affine=True), nn.LeakyReLU(0.2, True)]\n",
    "        model += [nn.Conv2d(128, 64, 3, 1, 1), nn.InstanceNorm2d(64, affine=True), nn.LeakyReLU(0.2, True)]\n",
    "        model += [nn.Conv2d(64, 32, 3, 1, 1), nn.InstanceNorm2d(32, affine=True), nn.LeakyReLU(0.2, True)]\n",
    "        model += [nn.Conv2d(32, 1, 3, 1, 1)]\n",
    "        self.model = nn.Sequential(*model)\n",
    "        self.gan_type = 'lsgan'\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "    def dis_attn(self, x):\n",
    "        out = self.forward(x)\n",
    "        attn = out.tanh().abs()\n",
    "        attn = attn.expand(x.size())\n",
    "        return attn * x + x\n",
    "\n",
    "    def calc_dis_loss(self, c_t, c_s):\n",
    "        out0 = self.forward(c_t)\n",
    "        out1= self.forward(c_s)\n",
    "        loss =0\n",
    "        if self.gan_type == 'lsgan':\n",
    "            loss += torch.mean((out0 - 0) ** 2) + torch.mean((out1 - 1) ** 2)\n",
    "        else:\n",
    "            assert 0, \"Unsupported GAN type: {}\".format(self.gan_type)\n",
    "        return loss\n",
    "\n",
    "    def calc_gen_loss(self, c_t):\n",
    "        # calculate the loss to train G\n",
    "        out0 = self.forward(c_t)\n",
    "        loss = 0\n",
    "        if self.gan_type == 'lsgan':\n",
    "            loss += torch.mean((out0 - 1)**2)\n",
    "        else:\n",
    "            assert 0, \"Unsupported GAN type: {}\".format(self.gan_type)\n",
    "        return loss\n",
    "\n",
    "class content_Dis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(content_Dis, self).__init__()\n",
    "        model = []\n",
    "        model += [nn.Conv2d(128, 128, 7, 2, 0), nn.InstanceNorm2d(128,affine=True), nn.LeakyReLU(0.2,True)]\n",
    "        model += [nn.Conv2d(128, 128, 7, 2, 0), nn.InstanceNorm2d(128,affine=True), nn.LeakyReLU(0.2,True)]\n",
    "        model += [nn.Conv2d(128, 128, 7, 2, 0), nn.InstanceNorm2d(128,affine=True), nn.LeakyReLU(0.2,True)]\n",
    "        model += [nn.Conv2d(128, 128, 7, 2, 0), nn.InstanceNorm2d(128,affine=True), nn.LeakyReLU(0.2,True)]\n",
    "        model += [nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=0)]\n",
    "        self.model = nn.Sequential(*model)\n",
    "        self.gan_type = 'lsgan'\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        out = out.view(-1)\n",
    "        outs = []\n",
    "        outs.append(out)\n",
    "        return outs\n",
    "\n",
    "    def calc_dis_loss(self, input0, input1):\n",
    "        outs0 = self.forward(input0)\n",
    "        outs1= self.forward(input1)\n",
    "        loss =0\n",
    "\n",
    "        for it, (out0, out1) in enumerate(zip(outs0, outs1)):\n",
    "            if self.gan_type == 'lsgan':\n",
    "                loss += torch.mean((out0 - 0)**2) + torch.mean((out1 - 1)**2)\n",
    "            elif self.gan_type == 'ralsgan':\n",
    "                loss += torch.mean((out1 - torch.mean(out0) - 1) ** 2) + torch.mean((out0 - torch.mean(out1) + 1) ** 2)\n",
    "            else:\n",
    "                assert 0, \"Unsupported GAN type: {}\".format(self.gan_type)\n",
    "        return loss\n",
    "\n",
    "    def calc_gen_loss(self, input0, input1):\n",
    "        # calculate the loss to train G\n",
    "        outs0 = self.forward(input0)\n",
    "        outs1= self.forward(input1)\n",
    "        loss = 0\n",
    "        for it, (out0, out1) in enumerate(zip(outs0, outs1)):\n",
    "            if self.gan_type == 'lsgan':\n",
    "                loss += torch.mean((out0 - 1)**2) + torch.mean((out1 - 0)**2)\n",
    "            elif self.gan_type == 'ralsgan':\n",
    "                loss += torch.mean((out0 - torch.mean(out1) - 1) ** 2) + torch.mean((out1 - torch.mean(out0) + 1) ** 2)\n",
    "            else:\n",
    "                assert 0, \"Unsupported GAN type: {}\".format(self.gan_type)\n",
    "        return loss\n",
    "\n",
    "class SegDis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegDis, self).__init__()\n",
    "        self.n_layer = 4\n",
    "        self.input_dim=3\n",
    "        self.dim=16\n",
    "        self.gan_type='lsgan'\n",
    "        self.netD=self._make_net()\n",
    "\n",
    "    def _make_net(self):\n",
    "        dim=16\n",
    "        cnn_x = []\n",
    "        cnn_x += [nn.Conv3d(self.input_dim, dim, kernel_size=(1,4,4),stride=(1,2,2),padding=(0,1,1)), nn.LeakyReLU(0.2, True)]\n",
    "        cnn_x += [nn.Conv3d(dim, 2*dim, kernel_size=(1,4,4),stride=(1,2,2),padding=(0,1,1)),nn.InstanceNorm3d(2*dim, affine=True), nn.LeakyReLU(0.2, True)]\n",
    "        cnn_x += [nn.Conv3d(2*dim, 4*dim, kernel_size=(4,4,4),stride=(2,2,2),padding=(1,1,1)),nn.InstanceNorm3d(4*dim,affine=True), nn.LeakyReLU(0.2, True)]\n",
    "        cnn_x += [nn.Conv3d(4*dim, 8*dim, kernel_size=(4,4,4),stride=(2,2,2),padding=(1,1,1)),nn.InstanceNorm3d(8*dim,affine=True), nn.LeakyReLU(0.2, True)]\n",
    "        cnn_x += [nn.Conv3d(8*dim, 8*dim, kernel_size=(4,4,4),stride=(2,2,2),padding=(1,1,1)),nn.InstanceNorm3d(8*dim,affine=True), nn.LeakyReLU(0.2, True)]\n",
    "        cnn_x += [nn.Conv3d(8*dim , 1, 1, 1, 0)]\n",
    "        return nn.Sequential(*cnn_x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return [self.netD(x)]\n",
    "\n",
    "    def calc_dis_loss(self, input_fake, input_real):\n",
    "        # calculate the loss to train D\n",
    "        input_real.requires_grad_()\n",
    "        outs0 = self.forward(input_fake)\n",
    "        outs1= self.forward(input_real)\n",
    "        loss =0\n",
    "\n",
    "        for it, (out0, out1) in enumerate(zip(outs0, outs1)):\n",
    "            if self.gan_type == 'lsgan':\n",
    "                loss += torch.mean((out0 - 0)**2) + torch.mean((out1 - 1)**2)\n",
    "            elif self.gan_type == 'nsgan':\n",
    "                all0 = Variable(torch.zeros_like(out0.data).cuda(), requires_grad=False)\n",
    "                all1 = Variable(torch.ones_like(out1.data).cuda(), requires_grad=False)\n",
    "                loss += torch.mean(F.binary_cross_entropy(F.sigmoid(out0), all0) +\n",
    "                                   F.binary_cross_entropy(F.sigmoid(out1), all1))\n",
    "            elif self.gan_type == 'ralsgan':\n",
    "                loss += torch.mean((out1 - torch.mean(out0) - 1) ** 2) + torch.mean((out0 - torch.mean(out1) + 1) ** 2)\n",
    "            else:\n",
    "                assert 0, \"Unsupported GAN type: {}\".format(self.gan_type)\n",
    "        return loss\n",
    "\n",
    "    def calc_gen_loss(self, input_fake):\n",
    "        # calculate the loss to train G\n",
    "        outs0 = self.forward(input_fake)\n",
    "\n",
    "        loss = 0\n",
    "        for it, out0 in enumerate(outs0):\n",
    "            if self.gan_type == 'lsgan':\n",
    "                loss += torch.mean((out0 - 1)**2) # LSGAN\n",
    "            elif self.gan_type == 'nsgan':\n",
    "                all1 = Variable(torch.ones_like(out0.data).cuda(), requires_grad=False)\n",
    "                loss += torch.mean(F.binary_cross_entropy(F.sigmoid(out0), all1))\n",
    "            elif self.gan_type == 'ralsgan':\n",
    "                loss += torch.mean((out0 - torch.mean(out1) - 1) ** 2) + torch.mean((out1 - torch.mean(out0) + 1) ** 2)\n",
    "            else:\n",
    "                assert 0, \"Unsupported GAN type: {}\".format(self.gan_type)\n",
    "        return loss\n",
    "\n",
    "class MsImageDis(nn.Module):\n",
    "    # Multi-scale discriminator architecture\n",
    "    def __init__(self):\n",
    "        super(MsImageDis, self).__init__()\n",
    "        self.gan_type = 'lsgan'\n",
    "        self.use_r1 = False\n",
    "        self.dim = 32\n",
    "        self.num_scales = 3\n",
    "        self.n_layer = 5\n",
    "        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
    "        self.cnns = nn.ModuleList()\n",
    "        for _ in range(self.num_scales):\n",
    "            self.cnns.append(self._make_net())\n",
    "\n",
    "    def _make_net(self):\n",
    "        dim = self.dim\n",
    "        cnn_x = []\n",
    "        cnn_x += [nn.Conv2d(1, dim, (4, 4), (2, 2), (1, 1)), nn.LeakyReLU(0.2, True)]\n",
    "        for i in range(self.n_layer - 1):\n",
    "            cnn_x += [nn.Conv2d(dim, min(2*dim,self.dim*8), (4, 4), (2, 2), (1, 1)), nn.InstanceNorm2d(min(2*dim,self.dim*8),affine=True), nn.LeakyReLU(0.2, True)]\n",
    "            dim =min(2*dim,self.dim*8)\n",
    "        cnn_x += [nn.Conv2d(dim , 1, 1, 1, 0)]\n",
    "        cnn_x = nn.Sequential(*cnn_x)\n",
    "        return cnn_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for model in self.cnns:\n",
    "            outputs.append(model(x))\n",
    "            x = self.downsample(x)\n",
    "        return outputs\n",
    "\n",
    "    def calc_dis_loss(self, input_fake, input_real):\n",
    "        # calculate the loss to train D\n",
    "        input_real.requires_grad_()\n",
    "        outs0 = self.forward(input_fake)\n",
    "        outs1= self.forward(input_real)\n",
    "        loss =0\n",
    "\n",
    "        for it, (out0, out1) in enumerate(zip(outs0, outs1)):\n",
    "            if self.gan_type == 'lsgan':\n",
    "                loss += torch.mean((out0 - 0)**2) + torch.mean((out1 - 1)**2)\n",
    "            elif self.gan_type == 'nsgan':\n",
    "                all0 = Variable(torch.zeros_like(out0.data).cuda(), requires_grad=False)\n",
    "                all1 = Variable(torch.ones_like(out1.data).cuda(), requires_grad=False)\n",
    "                loss += torch.mean(F.binary_cross_entropy(F.sigmoid(out0), all0) + F.binary_cross_entropy(F.sigmoid(out1), all1))\n",
    "            elif self.gan_type == 'ralsgan':\n",
    "                loss += torch.mean((out1 - torch.mean(out0) - 1) ** 2) + torch.mean((out0 - torch.mean(out1) + 1) ** 2)\n",
    "            else:\n",
    "                assert 0, \"Unsupported GAN type: {}\".format(self.gan_type)\n",
    "            if self.use_r1:\n",
    "                loss+= self.r1_reg(out1, input_real)\n",
    "        return loss\n",
    "\n",
    "    def calc_gen_loss(self, input_fake, input_real):\n",
    "        # calculate the loss to train G\n",
    "        outs0 = self.forward(input_fake)\n",
    "        outs1= self.forward(input_real)\n",
    "        loss = 0\n",
    "        for it, (out0, out1) in enumerate(zip(outs0, outs1)):\n",
    "            if self.gan_type == 'lsgan':\n",
    "                loss += torch.mean((out0 - 1)**2) # LSGAN\n",
    "            elif self.gan_type == 'nsgan':\n",
    "                all1 = Variable(torch.ones_like(out0.data).cuda(), requires_grad=False)\n",
    "                loss += torch.mean(F.binary_cross_entropy(F.sigmoid(out0), all1))\n",
    "            elif self.gan_type == 'ralsgan':\n",
    "                loss += torch.mean((out0 - torch.mean(out1) - 1) ** 2) + torch.mean((out1 - torch.mean(out0) + 1) ** 2)\n",
    "            else:\n",
    "                assert 0, \"Unsupported GAN type: {}\".format(self.gan_type)\n",
    "        return loss\n",
    "\n",
    "    def r1_reg(self, d_out, x_in):\n",
    "            # zero-centered gradient penalty for real images\n",
    "            batch_size = x_in.size(0)\n",
    "            grad_dout = torch.autograd.grad(\n",
    "                outputs=d_out.sum(), inputs=x_in,\n",
    "                create_graph=True, retain_graph=True, only_inputs=True\n",
    "            )[0]\n",
    "            grad_dout2 = grad_dout.pow(2)\n",
    "            assert (grad_dout2.size() == x_in.size())\n",
    "            reg = 0.5 * grad_dout2.view(batch_size, -1).sum(1).mean(0)\n",
    "            return reg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
